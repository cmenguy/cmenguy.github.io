{"pages":[{"url":"http://gradientdissent.com/pages/about.html","tags":"pages","title":"About","text":"I'm Charles Menguy, and I find it really hard to put a tag on myself as I enjoy software engineering as much as data science. My interests and experience lie along 3 main axes: Big Data : Experienced in dealing with multi-Terabyte datasets, both at the infrastructure as well as analytics levels. Data Science : Good knowledge of ML algorithms (both supervised and unsupervied) and when to use them, recently started looking at Deep Learning techniques and how to leverage it. Cloud Computing : I've been using AWS a lot and can work my way through pretty much any AWS service. Also looked into Google Compute Engine where I am still learning. I also have DevOps chops to help when needed configuring/deploying/maintaining Hadoop or Spark clusters. I currently work and live in the United States, in New York City, where I have spent most of my professional life. Initially graduated from EPITA in Paris, France, where I obtained a Master's degree in Computer Science, with a major in Cognitive Science. I enjoy more than anything traveling abroad and discovering new cultures. Tweets by @cmenguy !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/&#94;http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+\"://platform.twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);}}(document,\"script\",\"twitter-wjs\"); Not a huge social media freak so the best way to reach me is to contact me at my personal email address (hint: see on the side). I try to contribute to the open-source and data communities via the following channels: StackOverflow : I have been very active on the hadoop and related tags, have had less time to participate recently but still look at it at least once a day. Github : Github is awesome, I'm always on the lookout for new projects, and have a few small projects of my own. Planning to increase my rate of contributions in the very near future. InfoQ : I have been writing news on Big Data and Data Science topics at InfoQ. This involves direct contact with companies or organizations and a good understanding of the concepts involved. You can find most of the channels where I have a presence and interest in the navigation bar at the top, feel free to connect with me on any of those channels ! Always happy to discuss with like-minded folks."},{"url":"http://gradientdissent.com/pages/resume.html","tags":"pages","title":"Resume","text":"Charles Menguy Data engineer with 6+ years of professional experience in the online advertising industry. Fluent in Java and Python, intermediate in Scala and C++. Comfortable with Unix/shell scripting. Well-versed in big data technologies and applying data science techniques at scale in the cloud. Work Experience Since 2012 Senior Data Engineer for Adobe (New York City, United States). • Research and development of a cross-device stitching engine processing hundreds of Terabytes weekly via EMR and Spark using graph-based cluster algorithms, coordination via AWS Data Pipeline . Main development in Scala , analysis and evaluation via IPython notebooks . • Complete backend development of a customer segmentation platform from scratch in Java using Hadoop , HBase and Cassandra , managing more than 10 Billion profiles. This was done by leveraging Amazon Web Services in multiple regions, in particular EC2 , EMR and SQS . • Development of a Python -based data science platform to provide analytics on multi-dimensional data coming from different data sources with Hive and Redshift processing dozens of Terabytes daily. 150+ Terabytes stored. • Participation in the development of an algorithmic segmentation framework based on Hive using TF-IDF . • High-frequency distributed multi-threaded cookie exchange service built in Java with a custom streaming protocol based on SQS , making tens of thousands HTTP requests per second to dozens of ad networks/DSPs/DMPs. 2011-2012 Senior Software Engineer for Proclivity Media (New York City, United States). • Participation in the development of a scoring engine using Bayesian inference to compute business value of website visitors. • Using Hadoop and Pig to develop reporting infrastructure, and operational improvements. • Implementation of a job controller to standardize the company's Hadoop pipeline and scheduling. • High-frequency monitoring system to collect Hadoop metrics via ZeroMQ and analyzed via Esper to detect outliers and trends and visualized in Graphite . 2010-2011 Software Engineer for 24/7 Real Media (New York City, United States). • Implementation of scaling improvements on a custom C++ -based distributed system. • Research and development of customer segmentation engine in C++ . • Modernization of backend using MongoDB after extensive benchmarking of several NoSQL solutions. 2009 (5 months) Internship at GFI Informatique (Sophia-Antipolis, France). Research on image processing techniques to recognize QR codes on mobile and implementation of a mobile virtual shop using J2EE technologies. Open-Source Contributions Python Author of ndopt , a library for optimizing n-dimensional spaces using particle swarms. Author of griddle , a library for optimizing grid-like patterns using simulated annealing. Java Author of the cloudwatch-metrics library to collect Hadoop metrics into Amazon CloudWatch. Misc My StackOverflow reputation is close to 20k, among the top 2% contributors, and top 5 in the Hadoop community. Open Data I like taking open data apart to find something useful, and blog about it. Author of crime-analytics , an analysis of real crime data from San Francisco and Seattle to find temporal and geospatial patterns. Education 2007-2010 MS in Computer Science in EPITA (Paris, France). Top French school of Computer Science and Applied Mathematics with highly selective recruitment. Graduated with highest honors. Major in cognitive science and machine learning. 2005-2007 Preparatory classes in Lycée Chateaubriand (Rennes, France). Mathematics and Physics courses before a national competitive examination to Grandes Écoles. Equivalent to a US Bachelor's degree. 2005 High school diploma (SAT equivalent) with highest honors . Activities and Interests Hobbies I like traveling around the world and discovering new cultures and ways of thinking. I enjoy learning more about various topics, using platforms such as Coursera , Udacity or edX . Activities I have been teaching assistant at Coursera for several computer science and AI-related courses. Also a technical news editor at InfoQ focusing on the big data and data science topics. Recommendations Available upon request. charles@gradientdissent.com • +1 (347) 687-5697 • 28 years old • permanent resident 350 E 62nd Street, New York - NY 10065, United States www.gradientdissent.com"},{"url":"http://gradientdissent.com/blog/analyzing-2-months-of-real-crime-data-from-san-francisco-and-seattle.html","tags":"misc","title":"Analyzing 2 months of real crime data from San Francisco and Seattle","text":"The full data analysis described in this blog post can be found in the IPython notebook in the crime-analytics repository. I took part a couple weeks ago in the Coursera course Communicating Data Science Results as part of the Data Science at Scale specialization. It has been a great course - one of the assignments in particular was great because it was essentially just about taking a couple datasets, and coming up with our own problem and solution based on the data. The datasets in questions proposed were real crime data from San Francisco and Seattle . These being location-based datasets, I decided to take this opportunity to also get more familiar with the Python geospatial libraries such as GDAL and how it can be used in tandem with pandas DataFrames . Temporal Analysis Before looking at the geospatial distribution, I wanted to narrow it down to the crimes happening during the night since cursory analysis revealed that the type of crime has a high correlation with time of day (intuitively too). Initially I wanted to use seaborn 's heatmap for that, but the results were not aesthetically pleasing, so I decided to write my own heatmap system with matplotlib . For that purpose, here is a function below which creates a grid using simple matplotlib primitives: def preparePlot ( xticks , yticks , figsize = ( 10.5 , 6 ), hideLabels = False , gridColor = '#999999' , gridWidth = 1.0 ): plt . close () fig , ax = plt . subplots ( figsize = figsize , facecolor = 'white' , edgecolor = 'white' ) ax . axes . tick_params ( labelcolor = '#999999' , labelsize = '10' ) for axis , ticks in [( ax . get_xaxis (), xticks ), ( ax . get_yaxis (), yticks )]: axis . set_ticks_position ( 'none' ) axis . set_ticks ( ticks ) axis . label . set_color ( '#999999' ) if hideLabels : axis . set_ticklabels ([]) plt . grid ( color = gridColor , linewidth = gridWidth , linestyle = '-' ) map ( lambda position : ax . spines [ position ] . set_visible ( False ), [ 'bottom' , 'top' , 'left' , 'right' ]) return fig , ax Then to create the heatmap, we simply need to use matplotlib.pyplot.imshow with nearest interpolation. I also chose a grey-scale colormap to better represent the intensity of crimes at any given hour: plt . imshow ( img_src , interpolation = 'nearest' , aspect = 'auto' , cmap = cm . Greys ) The result shows a heatmap where the intensity is normalized across all hours, where a darker color represents a higher number of crimes at that hour, and lighter color represents a smaller number of crimes. There are some interesting patterns here, and we can see a few crime categories which seem to be particularly frequent at night. The main offenders seem to be assault , drunkenness , larceny/theft , prostitution , robbery and vehicle theft . These are the categories we'll focus on in order to have a meaningful geospatial visualization to see what San Francisco's crime scene looks like at night. Geospatial Analysis I had to research a few libraries to use in Python in order to visualize on a map the distribution of crimes. The complicated part is that I wanted to have access to the neighborhoods information, so I could break crimes down by neighborhood. Unfortunately, matplotlib 's Basemap toolkit doesn't provide much aside from 2D maps. A lot of inspiration for this analysis was drawn from this blog post about blue plaque in London . To solve it we need a Shapefile containing San Francisco's neighborhoods. Fortunately, there is one available on the SF open data portal and it can easily be used via the fiona module. shp = fiona . open ( \"/path/to/shapefile.shp\" ) From that we can easily compute the map boundaries w and h , and feed those to create a Basemap instance. We need a couple parameters to create our Basemap : The coordinates where our map should be centered. For San Francisco we use -122 , 37.7 The projection to use - in our case we use the transverse Mercator projection which should produce a map with less distortion since we are showing a relatively narrow geographic area. For more choice in projections, see the complete list The type of ellipsoid - now this one was a bit obscure to me, not being familiar with GIS, but this is actually nothing more than a coordinate system. It turns out WGS84 is the standard for GPS so it makes sense to use that in our case. We can put all that together to create a Basemap instance below: m = Basemap ( projection = 'tmerc' , lon_0 =- 122. , lat_0 = 37.7 , ellps = 'WGS84' , llcrnrlon = coords [ 0 ] - extra * w , llcrnrlat = coords [ 1 ] - extra + 0.01 * h , urcrnrlon = coords [ 2 ] + extra * w , urcrnrlat = coords [ 3 ] + extra + 0.01 * h , lat_ts = 0 , resolution = 'i' , suppress_ticks = True ) Now that we have our Basemap , we can add the content of our Shapefile containing San Francisco's neighborhoods: m . readshapefile ( '/path/to/shapefile.shp' , 'SF' , color = 'none' , zorder = 2 ) At that point, we need to start creating polygons for each neighborhood so that we can convert them into patches that can be represented on maps. Two particular libraries can help us achieve what we need: shapely is a library for analyzing and manipulating planar geometric objects in Python. In particular, it is useful to create Polygon and Point objects, which for us maps to neighborhoods and crime occurrences. descartes is the intermediary between shapely's polygons, and matplotlib's maps. It can be used to create PolygonPatch patches which can be represented on a Basemap . For that purpose we create a DataFrame where for each neighborhood we will have a PolygonPatch based on Polygon objects extracted from the content of the Shapefile. df_map = pd . DataFrame ({ 'poly' : [ Polygon ( xy ) for xy in m . SF ], 'ward_name' : [ ward [ 'name' ] for ward in m . SF_info ]}) df_map [ 'area_m' ] = df_map [ 'poly' ] . map ( lambda x : x . area ) df_map [ 'area_km' ] = df_map [ 'area_m' ] / 100000 # Draw neighborhoods with polygons df_map [ 'patches' ] = df_map [ 'poly' ] . map ( lambda x : PolygonPatch ( x , fc = '#000000' , ec = '#ffffff' , lw =. 5 , alpha = 1 , zorder = 4 )) Now we're ready to draw the crime occurrences on the map. Since our data contains Latitude and Longitude columns, we can easily draw them after making sure they're within the polygon formed by San Francisco's neighborhoods. For example, to draw all occurrences of vehicle theft: m . scatter ( [ geom . x for geom in sf_night_vehicle_theft_points ], [ geom . y for geom in sf_night_vehicle_theft_points ], 10 , marker = 'o' , lw =. 25 , facecolor = 'cyan' , edgecolor = 'cyan' , alpha = 0.75 , antialiased = True , label = 'Vehicle Theft' , zorder = 3 ) And of course we need to draw the neighborhoods polygons on the map as well, in order to visualize the distribution of crimes in the city. This can be done using a matplotlib.collections.PatchCollection which can be created from the neighborhood patches computed previously. ax . add_collection ( PatchCollection ( df_map [ 'patches' ] . values , match_original = True )) The final result is a map of San Francisco with all neighborhood boundaries and crimes represented with different colors for the 6 categories of crimes that happen mostly at night. We can get a general feeling that the center of the city (especially the Tenderloin) as well as the south are pretty agitated at night, while the west area is mostly quiet at night in terms of crimes. But this just gives us a general picture, when we would like to clearly see for each neighborhood how criminal it is. Of course this is a little biased towards highly-populated neighborhoods, but would still give a good enough idea where to be careful at night. Before we can divide neighborhoods in criminality level, we need to compute the crime density per neighborhood. This can easily be done by identifying, for each crime occurrence, in which neighborhood it occurred, count those, and deduce the crime density. df_map [ 'count' ] = df_map [ 'poly' ] . map ( lambda x : int ( len ( filter ( prep ( x ) . contains , sf_night_crimes_points )))) df_map [ 'density_m' ] = df_map [ 'count' ] / df_map [ 'area_m' ] df_map [ 'density_km' ] = df_map [ 'count' ] / df_map [ 'area_km' ] We can then apply a clustering algorithm to group neighborhoods into N criminality buckets. In our case, since there are not that many neighborhoods, 5 groups seem good enough. The algorithm we're using here is Jenks natural breaks optimization . This is a pretty common method used in cartography software, and is available as part of the PySAL library. from pysal.esda.mapclassify import Natural_Breaks as nb breaks = nb ( df_map [ df_map [ 'density_km' ] . notnull ()] . density_km . values , initial = 300 , k = 5 ) Once the algorithm converges, we have multiple bins that we need to join back to our original neighborhood patches to figure out to which bin each neighborhood belongs. jb = pd . DataFrame ({ 'jenks_bins' : breaks . yb }, index = df_map [ df_map [ 'density_km' ] . notnull ()] . index ) df_map . drop ( \"jenks_bins\" , inplace = True , axis = 1 , errors = \"ignore\" ) df_map = df_map . join ( jb ) All that is left at that point is to draw again our PatchCollection , with the small trick of using the set_facecolor method to apply a color corresponding to the cluster. We used the Blues colormap in this case. cmap = plt . get_cmap ( 'Blues' ) pc = PatchCollection ( df_map [ 'patches' ], match_original = True ) norm = Normalize () pc . set_facecolor ( cmap ( norm ( df_map [ 'jenks_bins' ] . values ))) ax . add_collection ( pc ) This was a pretty fun analysis, and a good introduction to geospatial analysis in Python with fiona , descartes , shapely , Basemap and PySAL . We're barely scratching the surface here by looking at only 2 months worth of data - I might do a follow-up later with more data since we have daily data since January 2003 which is a lot more crime data to analyze."},{"url":"http://gradientdissent.com/blog/yet-another-data-blog-in-pelican.html","tags":"misc","title":"Yet Another Data Blog in Pelican","text":"Python and Data go very well together these days, so I've decided to move from Octopress to Pelican. It was becoming a bit embarrassing not being able to hack the internals of my blog (not that I have anything against Ruby, but it's just not one of my primary languages). So after a gap of almost 2 years, I'm officially running on Pelican, and so far I'm thrilled about what it brings to the table. Why Pelican, you might ask? And what even is Pelican? Let me try to answer all of that in this inaugural post, which will I hope offer some insight to other folks looking at static websites. Why I said no to Octopress So, like I said, Octopress being in Ruby has always held me down, but it wasn't the only issue. The whole transition to Octopress 3.0 seemed a bit confusing to me, and I was reading many blog posts also complaining about it. Feature\\Engine Octopress Pelican Nikola Community 9,300 stars, 3,100 forks 5,400 stars, 1,200 forks 920 stars, 250 forks Language Ruby Python Python Themes a lot 100+ 30+ Deployment Mainly GH pages, more in 3.0 GH pages, S3, Dropbox, FTP, CloudFiles, ... Anything, but commands have to be specified manually IPython notebook support Plugin Plugin Native I finally settled for Pelican, because it had a pretty good community behind it, a decent amount of themes available, decent support for IPython notebook and, well, it's Python-based ! Nikola looked really good too, and some of their themes were outstanding, but I was a bit worried about the lack of community behind it. Also the fact that I had trouble getting it to work on Python 2.7 made me a little bit worried. Crafting the right look & feel One thing I've been struggling with Pelican, is finding the right theme for my blog. The pelican-bootstrap3 blog looks pretty good, but I also really liked the BT3-flat , but I kinda wanted something in the middle, simple yet easy on the eyes. To that end I've extended the pelican-bootstrap3 theme to fit my needs. I'm really really not a front-end person so it has been quite a bore, but I'm quite pleased with the final result (and hope I never have to touch this ever !) First thing I wanted was the social buttons in the top bar instead of on the side - they look really out-of-place in bootstrap3, and to me felt much more natural at the top. For that, I modified pelican-bootstrap3/templates/base.html like so: {% if SOCIAL and INCLUDE_SOCIAL_IN_NAVBAR %} <!-- social media icons --> < ul class = \"nav navbar-nav navbar-right social\" > {% for s in SOCIAL %} {% if s[2] %} {% set name_sanitized = s[2]|lower|replace('+','-plus')|replace(' ','-') %} {% else %} {% set name_sanitized = s[0]|lower|replace('+','-plus')|replace(' ','-') %} {% endif %} {% set iconattributes = '\"fa fa-lg fa-' ~ name_sanitized ~ '\"' %} < li >< a href = \"{{ s[1] }}\" >< i class = {{ iconattributes }} ></ i ></ a ></ li > {% endfor %} </ ul > {% endif %} I also wanted to use the Pelican about me facilities, but found they rendered pretty poorly in bootstrap3. For me a good about me element should contain the following: Personal information List of active tags A downloadable resume (we'll discuss that in more details in the next section) I enclosed the about me section in a well-sm class in pelican-bootstrap3/templates/includes/aboutme.html : < div id = \"aboutme\" class = \"well well-sm\" > Then inside that, I made it so every field can be filled from pelicanconf.py optionally: {% if ABOUT_ME_TOPICS %} < div class = \"sub-name\" > {{ ABOUT_ME_TOPICS }} </ div > {% endif %} {% if ABOUT_ME_EMAIL %} < div class = \"contact\" > {{ ABOUT_ME_EMAIL }} </ div > {% endif %} {% if ABOUT_ME_LOCATION %} < div class = \"location\" > {{ ABOUT_ME_LOCATION }} </ div > {% endif %} At that point it's super easy to control your about me from the Pelican config. For example, here is what I have: ABOUT_ME = True ABOUT_ME_TOPICS = \"big data | data science | cloud computing\" ABOUT_ME_EMAIL = \"menguy.charles@gmail.com\" ABOUT_ME_LOCATION = \"New York City, United States\" Regarding the tags, I took them from the sidebar and added them in the about me section instead inline, which looked more compact and aesthetically pleasing than in the sidebar to me: {% if 'tag_cloud' in PLUGINS and DISPLAY_TAGS_ON_ABOUTME %} {% if DISPLAY_TAGS_INLINE %} {% set tags = tag_cloud | sort(attribute='0') %} {% else %} {% set tags = tag_cloud | sort(attribute='1') %} {% endif %} < br /> < ul class = \"list-group {% if DISPLAY_TAGS_INLINE %}list-inline tagcloud{% endif %}\" id = \"tags\" > {% for tag in tag_cloud %} < li class = \"list-group-item tag-{{ tag.1 }}\" > < a class = \"tag\" href = \"{{ SITEURL }}/{{ tag.0.url }}\" > #{{ tag.0 }} </ a > </ li > {% endfor %} </ ul > {% endif %} And to top it off, I created a CUSTOM_CSS to adjust things a little bit and entered that in pelicanconf.py . If you like the look & feel of this blog, feel free to reuse my code - I haven't made it a separate project because it feels kind of an hybrid between two pre-existing themes, but if there is demand I can always create a separate repo for it: git clone -b sources git@github.com:cmenguy/cmenguy.github.io.git Resume Automation One thing that I wanted out of this blog is an easy, low-maintenance and automated way to update and share my resume. An HTML is totally fine these days, but you also need a PDF resume in most cases, and that's where it gets tricky: oftentimes they PDF and HTML get out-of-sync, creating the PDF goes through a different channel so you end up having to replicate what you already wrote in HTML. To solve that, I wrote a Pelican plugin called pelican-resume to take care of this automatically. With it, everytime you run pelican content , if you have a Markdown file under pages/resume.md it will automatically create a PDF resume that can be embedded into your Pelican blog. How to install it? Easy, with pip: pip install pelican-resume How to use it? Simply include it in your list of plugins in pelicanconf.py : PLUGINS = [ # ... \"pelican_resume\" , # ... ] The default settings will take your resume.md and create a PDF resume using the moderncv style under pdfs/resume.pdf in your OUTPUT_PATH . All of this can be customized, and you are welcome to look at the README in the repository for further information. Feel free to also contribute to it if you have custom CSS for different resume styles, if you want to support something else than Markdown in input, ... Automation & Ease of use One of the things I hated with Octopress, is the fact that I constantly ran into conflicts when deploying to master, and it made me want to cry. Here, I'm actually using Travis to automatically and continuously deploy on every push without having to deal with any of the manual work myself, which is a more than welcome change. All of the code samples shown below are contained in the .travis.yml file which is placed at the root of the repository in the branch containing your Pelican sources (I called mine sources ). The first thing we need to tell Travis is that it should only build in the sources branch, and not in master: branches : only : - sources Another thing I want is to be notified on every failure or success - I usually don't, but for a blog I like to be extra sure that things are working ok when I push a new article. notifications : email : on_success : always on_failure : always Next the main issue I had with Travis was about installing everything required to build the Pelican blog. There's actually a bunch of packages needed here: Python modules: pelican, markdown, beautifulsoup4, IPython, ghp-import and pelican-resume (see requirements.txt ) Pandoc - somehow if we install it via Travis' apt_packages it uses an old version which produces poor formatting, so we need to install it manually Wkhtmltopdf - this is not even available via apt-get for Travis, so it needs to be installed manually The code snippet below contains the necessary commands to install all these dependencies on Travis: install : # Install required Python modules - pip install -r requirements.txt # Install pandoc manually since it's disallowed with apt-get in Travis - mkdir $HOME/pandoc - curl -O https://s3.amazonaws.com/rstudio-buildtools/pandoc-1.12.3.zip - unzip -j pandoc-1.12.3.zip pandoc-1.12.3/linux/debian/x86_64/pandoc -d $HOME/pandoc - chmod +x $HOME/pandoc/pandoc - rm pandoc-1.12.3.zip - export PATH=\"$HOME/pandoc:$PATH\" - pandoc --version # Install wkhtmltopdf manually since not available in apt-get - mkdir wkhtmltopdf - wget http://download.gna.org/wkhtmltopdf/0.12/0.12.3/wkhtmltox-0.12.3_linux-generic-amd64.tar.xz - tar --xz -xvf wkhtmltox-0.12.3_linux-generic-amd64.tar.xz -C wkhtmltopdf - export PATH=$PATH:$PWD/wkhtmltopdf/wkhtmltox/bin/ At that point Travis has everything it needs to build the blog and it just needs to run a make publish . I also save the log and place it in the output so I can easily see what happened in case something goes wrong: script : - pip freeze; make DEBUG=1 publish 2>&1 | tee -a build.log; cp build.log output/build.log And finally it needs to publish to Github in case of success. This part is a little bit tricky, because we need to pass an authentication token to Travis so it can push directly to the master branch. To do that I've modified the Makefile to use the GH_TOKEN whose encrypted value is passed in .travis.yml via the env.global.secure field: after_success : make github env : global : secure : <encrypted-gh-token> To encrypt the token, you need to create one in the Github UI, and then encrypt it via the travis gem: gem install travis travis encrypt GH_TOKEN=<gh-token> And voila ! With this simple configuration, you get a fully automated blog where you only need to fill your Markdown pages, and Travis will take care of generating the relevant HTML and producing your PDF resume out of it. With that system in place it should be extremely easy for me to keep updating my blog without wanting to kill myself everytime I need to deploy something live. Now that the boring UI thing is out of the way, I pledge to write at least 1 article per month on various data topics. Some things I'm planning to focus on in this blog: Looking at Open Data, finding some insights and sharing them. Latest Big Data trends, and how they can be applied in a Data Science context. A new look at existing Big Data technologies. Stay tuned for more !"}]}